# The Seven Faces of Stress: Understanding Facial Activity Patterns during Cognitive Stress
Carla Viegas, Roy Maxion, Alexander Hauptmann, and Joa達o Magalh達es

# Proposed Method

![alt text](https://github.com/clviegas/SevenFacesOfStress/blob/main/diagram_methods.png?raw=true)


# Results

![alt text](https://github.com/[username]/[reponame]/blob/[branch]/seven_faces_stress.png?raw=true)



## Data access
Please reach out to maxion@cs.cmu.edu to request access to the data.

If you use the data or find this project useful please cite the following papers:
```
@INPROCEEDINGS{8516497,
  author={Viegas, Carla and Maxion, Roy and Hauptmann, Alexander and Magalh達es, Jo達o},
  booktitle={2024 18th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2024)}, 
  title={The Seven Faces of Stress: Understanding Facial Activity Patterns during Cognitive Stress}, 
  year={2024},
  volume={},
  number={},
  pages={},
  keywords={Stress;Lips;Stress measurement;Monitoring;Biomedical monitoring;Gold;Data visualization;affect recognition;stress detection;facial action units;person independent model;person dependent model},
  doi={}}

@INPROCEEDINGS{8516497,
  author={Viegas, Carla and Lau, Shing-Hon and Maxion, Roy and Hauptmann, Alexander},
  booktitle={2018 International Conference on Content-Based Multimedia Indexing (CBMI)}, 
  title={Towards Independent Stress Detection: A Dependent Model Using Facial Action Units}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  keywords={Stress;Lips;Stress measurement;Monitoring;Biomedical monitoring;Gold;Data visualization;affect recognition;stress detection;facial action units;person independent model;person dependent model},
  doi={10.1109/CBMI.2018.8516497}}

@inproceedings{10.1145/3281151.3281158,
author = {Viegas, Carla and Lau, Shing-Hon and Maxion, Roy and Hauptmann, Alexander},
title = {Distinction of stress and non-stress tasks using facial action units},
year = {2018},
isbn = {9781450360029},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281151.3281158},
doi = {10.1145/3281151.3281158},
abstract = {Long-exposure to stress is known to lead to physical and mental health problems. But how can we as individuals track and monitor our stress? Wearables which measure heart variability have been studied to detect stress. Such devices, however, need to be worn all day long and can be expensive. As an alternative, we propose the use of frontal face videos to distinguish between stressful and non-stressful activities. Affordable personal tracking of stress levels could be obtained by analyzing the video stream of inbuilt cameras in laptops. In this work, we present a preliminary analysis of 114 one-hour long videos. During the video, the subjects perform a typing exercise before and after being exposed to a stressor. We performed a binary classification using Random Forest (RF) to distinguish between stressful and non-stressful activities. As features, facial action units (AUs) extracted from each video frame were used. We obtained an average accuracy of over 97\% and 50\% for subject dependent and subject independent classification, respectively.},
booktitle = {Proceedings of the 20th International Conference on Multimodal Interaction: Adjunct},
articleno = {7},
numpages = {6},
keywords = {affective computing, facial action units, stress detection},
location = {Boulder, Colorado},
series = {ICMI '18}
}


```
